Project Overview
This project leverages Apache PySpark for large-scale data analytics and Apache Airflow for workflow orchestration. The goal is to analyze a housing dataset to derive insights into property pricing, square footage, house condition, and waterfront access. The analytics results are stored locally and uploaded to Google Drive for easy access.

Table of Contents
Features
Tech Stack
Installation and Setup
Prerequisites
Configuration
Data Flow
Usage
Running the DAG
Viewing Outputs
Folder Structure
Contributing
License
Features
Price Statistics by Zipcode: Aggregates price statistics (mean, max, min, and house count) for each zipcode.
House Features Analysis: Analyzes average price and square footage by the number of bedrooms.
Condition and Grade Distribution: Generates statistics on house condition and grade.
Waterfront Analysis: Analyzes waterfront properties, showing average price, count, and view score.
Tech Stack
Apache PySpark - For large-scale data processing.
Apache Airflow - To manage and automate the ETL pipeline.
Google Drive API - For uploading output files to Google Drive.
Python - Core programming language.
Google OAuth2 Credentials - For authenticating Google Drive API.
Installation and Setup
Prerequisites
Python 3.8+

Apache Spark 3.x

Apache Airflow 2.x

Google Drive API Credentials

Set up a project on Google Cloud and enable the Drive API.
Download credentials.json and save it as token.json in the Airflow DAG directory.
Install required Python packages:

pip install apache-airflow apache-airflow-providers-google apache-spark google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client
Configuration
Airflow Setup
Ensure Apache Airflow is correctly installed, configured, and running.
Start Airflowâ€™s web server and scheduler to enable DAG scheduling and monitoring.
Google API Setup
Set up Google Drive API on Google Cloud Console, and download the credentials.json file.
Place this file in /home/vboxuser/airflow/dags/ and rename it to token.json for authentication and Drive API integration.
Data File
Save the housing dataset as house_data.dat in /home/vboxuser/airflow/dags/ for access by the PySpark tasks within Airflow.
Data Flow
DAG Setup: Airflow runs the DAG daily, calling the perform_analytics task.
Data Processing: PySpark reads, cleans, and processes the data from house_data.dat.
Analysis Generation: The data undergoes various aggregations and analyses as defined in perform_analytics().
Local Storage: Intermediate results are saved in CSV format in the analytics sub-directory.
Consolidation: Small CSV parts are consolidated into single CSV files for each analysis type.
Google Drive Upload: Consolidated CSV files are uploaded to Google Drive for easy access and sharing.
Usage
Running the DAG
Start the Airflow scheduler:
airflow scheduler
Usage
Running the DAG
To trigger the DAG from the Airflow web UI, navigate to the "DAGs" section, find home_schedule_dag, and click on the "Trigger DAG" button. Alternatively, you can start it manually from the command line:

airflow dags trigger home_schedule_dag
Viewing Outputs
The consolidated CSV files are saved locally in /home/vboxuser/airflow/dags/analytics/. These files will also be uploaded to the specified Google Drive folder (configure the folder ID in the perform_analytics function within the DAG script).
